//
//  AudioFileProcessor.swift
//  openai-transcriber-ios
//
//  Created by apple on 2025/05/24.
//

import Foundation
import AVFoundation
import Accelerate

/// éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ç„¡éŸ³ã§åˆ†å‰²ã™ã‚‹ãƒ—ãƒ­ã‚»ãƒƒã‚µ
final class AudioFileProcessor: ObservableObject {
    
    // MARK: - Types
    struct ProcessingResult {
        let segments: [(url: URL, startTime: TimeInterval, duration: TimeInterval)]
        let totalDuration: TimeInterval
    }
    
    enum ProcessingError: Error {
        case fileNotFound
        case unsupportedFormat
        case conversionFailed
        case processingFailed(String)
    }
    
    // MARK: - Properties
    @Published var isProcessing = false
    @Published var progress: Double = 0.0
    
    private let silenceThreshold: Float = 0.01  // RMSé–¾å€¤
    private let silenceWindow: TimeInterval = 0.5  // ç„¡éŸ³åˆ¤å®šæ™‚é–“
    private let minSegmentDuration: TimeInterval = 0.5  // æœ€å°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·
    private let outputFormat: AVAudioFormat
    
    // MARK: - Initialization
    init() {
        // Whisperç”¨ã®å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆ16kHz, 16bit, monoï¼‰
        self.outputFormat = AVAudioFormat(
            commonFormat: .pcmFormatInt16,
            sampleRate: 16_000,
            channels: 1,
            interleaved: true
        )!
    }
    
    // MARK: - Public Methods
    
    /// éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ç„¡éŸ³ã§åˆ†å‰²
    func processFile(at url: URL) async throws -> ProcessingResult {
        await MainActor.run { 
            self.isProcessing = true 
            self.progress = 0.0
        }
        
        defer {
            Task { @MainActor in
                self.isProcessing = false
            }
        }
        
        // ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ãƒ¼ãƒ—ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹
        guard url.startAccessingSecurityScopedResource() else {
            throw ProcessingError.fileNotFound
        }
        defer { url.stopAccessingSecurityScopedResource() }
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
        let file = try AVAudioFile(forReading: url)
        let inputFormat = file.processingFormat
        let totalFrames = AVAudioFrameCount(file.length)
        let totalDuration = Double(totalFrames) / inputFormat.sampleRate
        
        print("ğŸ“ Processing file: \(url.lastPathComponent)")
        print("ğŸ“Š Format: \(inputFormat)")
        print("â±ï¸ Duration: \(totalDuration)s")
        
        // ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
        let needsConversion = inputFormat.sampleRate != outputFormat.sampleRate ||
                            inputFormat.channelCount != outputFormat.channelCount
        
        // ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºï¼ˆ0.1ç§’åˆ†ï¼‰
        let bufferSize = AVAudioFrameCount(inputFormat.sampleRate * 0.1)
        
        // ç„¡éŸ³æ¤œå‡ºç”¨ã®å¤‰æ•°
        var segments: [(url: URL, startTime: TimeInterval, duration: TimeInterval)] = []
        var currentSegmentStart: TimeInterval? = nil
        var currentSegmentFrames: [AVAudioPCMBuffer] = []
        var lastSpeechTime: TimeInterval = 0
        var currentTime: TimeInterval = 0
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ãªãŒã‚‰å‡¦ç†
        while file.framePosition < totalFrames {
            // ãƒãƒƒãƒ•ã‚¡ã‚’ä½œæˆ
            let remainingFrames = totalFrames - AVAudioFrameCount(file.framePosition)
            let framesToRead = min(bufferSize, remainingFrames)
            
            guard let buffer = AVAudioPCMBuffer(
                pcmFormat: inputFormat,
                frameCapacity: framesToRead
            ) else { continue }
            
            // ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
            try file.read(into: buffer, frameCount: framesToRead)
            
            // RMSè¨ˆç®—
            let rms = calculateRMS(buffer: buffer)
            let isSpeech = rms > silenceThreshold
            
            // ç™ºè©±æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯
            if isSpeech {
                // ç™ºè©±é–‹å§‹
                if currentSegmentStart == nil {
                    currentSegmentStart = currentTime
                    currentSegmentFrames = []
                    print("ğŸ¤ Speech started at \(currentTime)s")
                }
                currentSegmentFrames.append(buffer)
                lastSpeechTime = currentTime
                
            } else if let segmentStart = currentSegmentStart {
                // ç„¡éŸ³æ¤œå‡º
                let silenceDuration = currentTime - lastSpeechTime
                
                if silenceDuration >= silenceWindow {
                    // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç¢ºå®š
                    let segmentDuration = lastSpeechTime - segmentStart
                    
                    if segmentDuration >= minSegmentDuration {
                        // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜
                        if let segmentURL = try await saveSegment(
                            frames: currentSegmentFrames,
                            inputFormat: inputFormat,
                            startTime: segmentStart,
                            duration: segmentDuration
                        ) {
                            segments.append((
                                url: segmentURL,
                                startTime: segmentStart,
                                duration: segmentDuration
                            ))
                            print("ğŸ’¾ Saved segment: \(segmentStart)s - \(lastSpeechTime)s")
                        }
                    }
                    
                    // ãƒªã‚»ãƒƒãƒˆ
                    currentSegmentStart = nil
                    currentSegmentFrames = []
                }
            }
            
            // æ™‚é–“ã‚’æ›´æ–°
            currentTime += Double(buffer.frameLength) / inputFormat.sampleRate
            
            // é€²æ—æ›´æ–°
            await MainActor.run {
                self.progress = currentTime / totalDuration
            }
        }
        
        // æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†
        if let segmentStart = currentSegmentStart {
            let segmentDuration = currentTime - segmentStart
            if segmentDuration >= minSegmentDuration {
                if let segmentURL = try await saveSegment(
                    frames: currentSegmentFrames,
                    inputFormat: inputFormat,
                    startTime: segmentStart,
                    duration: segmentDuration
                ) {
                    segments.append((
                        url: segmentURL,
                        startTime: segmentStart,
                        duration: segmentDuration
                    ))
                }
            }
        }
        
        print("âœ… Processing complete: \(segments.count) segments")
        
        return ProcessingResult(
            segments: segments,
            totalDuration: totalDuration
        )
    }
    
    // MARK: - Private Methods
    
    /// ãƒãƒƒãƒ•ã‚¡ã®RMSã‚’è¨ˆç®—
    private func calculateRMS(buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData else { return 0 }
        
        var rms: Float = 0
        let channelCount = Int(buffer.format.channelCount)
        let frameLength = Int(buffer.frameLength)
        
        // å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã®RMSã‚’å¹³å‡
        for channel in 0..<channelCount {
            var channelRMS: Float = 0
            vDSP_rmsqv(channelData[channel], 1, &channelRMS, vDSP_Length(frameLength))
            rms += channelRMS
        }
        
        return rms / Float(channelCount)
    }
    
    /// ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    private func saveSegment(
        frames: [AVAudioPCMBuffer],
        inputFormat: AVAudioFormat,
        startTime: TimeInterval,
        duration: TimeInterval
    ) async throws -> URL? {
        
        guard !frames.isEmpty else { return nil }
        
        // ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«URL
        let tempURL = FileManager.default.temporaryDirectory
            .appendingPathComponent("segment_\(UUID().uuidString).wav")
        
        // å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        let outputFile = try AVAudioFile(
            forWriting: tempURL,
            settings: outputFormat.settings,
            commonFormat: outputFormat.commonFormat,
            interleaved: outputFormat.isInterleaved
        )
        
        // ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ãŒå¿…è¦ãªå ´åˆ
        if inputFormat != outputFormat {
            guard let converter = AVAudioConverter(from: inputFormat, to: outputFormat) else {
                throw ProcessingError.conversionFailed
            }
            
            // å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å¤‰æ›ã—ã¦æ›¸ãè¾¼ã¿
            for frame in frames {
                let outputFrameCapacity = AVAudioFrameCount(
                    Double(frame.frameLength) * outputFormat.sampleRate / inputFormat.sampleRate
                )
                
                guard let outputBuffer = AVAudioPCMBuffer(
                    pcmFormat: outputFormat,
                    frameCapacity: outputFrameCapacity
                ) else { continue }
                
                var error: NSError?
                let status = converter.convert(to: outputBuffer, error: &error) { _, outStatus in
                    outStatus.pointee = .haveData
                    return frame
                }
                
                if status == .error {
                    print("âš ï¸ Conversion error: \(error?.localizedDescription ?? "Unknown")")
                    continue
                }
                
                try outputFile.write(from: outputBuffer)
            }
        } else {
            // ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ä¸è¦ãªå ´åˆã¯ç›´æ¥æ›¸ãè¾¼ã¿
            for frame in frames {
                try outputFile.write(from: frame)
            }
        }
        
        return tempURL
    }
    
    /// ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    func cleanupTemporaryFiles(urls: [URL]) {
        for url in urls {
            try? FileManager.default.removeItem(at: url)
        }
    }
}